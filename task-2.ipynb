{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10488249,"sourceType":"datasetVersion","datasetId":6409347}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## All necessary imports","metadata":{}},{"cell_type":"code","source":"!pip install googletrans==4.0.0-rc1\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nimport os\nimport re\nimport random\nfrom datasets import load_dataset, Audio, DatasetDict\nfrom collections import Counter\nimport torch\nfrom sklearn.metrics import precision_recall_fscore_support, accuracy_score\nfrom sklearn.metrics import f1_score\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.metrics import AUC\n\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom googletrans import Translator\nfrom IPython.display import clear_output\nfrom transformers import TrainerCallback\nfrom transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\nfrom sklearn.model_selection import train_test_split\nimport tempfile\n\nfrom IPython.display import Audio as IPythonAudio\nimport librosa\nfrom transformers import pipeline, TrainingArguments, Trainer, AutoModelForSequenceClassification\nfrom transformers import Wav2Vec2Processor, Wav2Vec2ForSequenceClassification\nfrom torch.utils.data import Dataset as TorchDataset\nfrom transformers import DataCollatorWithPadding\nimport traceback\nfrom datasets import Dataset as hgdataset\n\nfrom transformers import TFBertModel, TFWav2Vec2Model\nfrom transformers import TFDistilBertModel\nfrom tensorflow.keras.layers import *\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom tensorflow.keras.models import Model\n\n\nfrom transformers.utils import logging\nlogging.set_verbosity_error()\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-29T03:22:07.412483Z","iopub.execute_input":"2025-01-29T03:22:07.412818Z","iopub.status.idle":"2025-01-29T03:22:38.465646Z","shell.execute_reply.started":"2025-01-29T03:22:07.412790Z","shell.execute_reply":"2025-01-29T03:22:38.464867Z"}},"outputs":[{"name":"stdout","text":"Collecting googletrans==4.0.0-rc1\n  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting httpx==0.13.3 (from googletrans==4.0.0-rc1)\n  Downloading httpx-0.13.3-py3-none-any.whl.metadata (25 kB)\nRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2024.12.14)\nCollecting hstspreload (from httpx==0.13.3->googletrans==4.0.0-rc1)\n  Downloading hstspreload-2025.1.1-py3-none-any.whl.metadata (2.1 kB)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.1)\nCollecting chardet==3.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n  Downloading chardet-3.0.4-py2.py3-none-any.whl.metadata (3.2 kB)\nCollecting idna==2.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\nCollecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==4.0.0-rc1)\n  Downloading rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\nCollecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n  Downloading httpcore-0.9.1-py3-none-any.whl.metadata (4.6 kB)\nCollecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n  Downloading h11-0.9.0-py2.py3-none-any.whl.metadata (8.1 kB)\nCollecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n  Downloading h2-3.2.0-py2.py3-none-any.whl.metadata (32 kB)\nCollecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n  Downloading hyperframe-5.2.0-py2.py3-none-any.whl.metadata (7.2 kB)\nCollecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n  Downloading hpack-3.0.0-py2.py3-none-any.whl.metadata (7.0 kB)\nDownloading httpx-0.13.3-py3-none-any.whl (55 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading idna-2.10-py2.py3-none-any.whl (58 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\nDownloading hstspreload-2025.1.1-py3-none-any.whl (1.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\nDownloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\nBuilding wheels for collected packages: googletrans\n  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17397 sha256=09412e2358e7bd72827eb0fa87d0aad9454bbcefce8116adf6353d929ba12a2b\n  Stored in directory: /root/.cache/pip/wheels/c0/59/9f/7372f0cf70160fe61b528532e1a7c8498c4becd6bcffb022de\nSuccessfully built googletrans\nInstalling collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n  Attempting uninstall: h11\n    Found existing installation: h11 0.14.0\n    Uninstalling h11-0.14.0:\n      Successfully uninstalled h11-0.14.0\n  Attempting uninstall: chardet\n    Found existing installation: chardet 5.2.0\n    Uninstalling chardet-5.2.0:\n      Successfully uninstalled chardet-5.2.0\n  Attempting uninstall: idna\n    Found existing installation: idna 3.10\n    Uninstalling idna-3.10:\n      Successfully uninstalled idna-3.10\n  Attempting uninstall: httpcore\n    Found existing installation: httpcore 1.0.7\n    Uninstalling httpcore-1.0.7:\n      Successfully uninstalled httpcore-1.0.7\n  Attempting uninstall: httpx\n    Found existing installation: httpx 0.28.1\n    Uninstalling httpx-0.28.1:\n      Successfully uninstalled httpx-0.28.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlangsmith 0.2.3 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\nopenai 1.57.4 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\ntensorflow-decision-forests 1.10.0 requires tensorflow==2.17.0, but you have tensorflow 2.17.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed chardet-3.0.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2025.1.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## Text file loading","metadata":{}},{"cell_type":"code","source":"df_malayalam = pd.read_excel(\"/kaggle/input/shared-task-hunt/Train set/tamil/text/TA-AT-train.xlsx\")\n\ndf_test_malayalam = pd.read_excel(\"/kaggle/input/shared-task-hunt/Test set/tamil/text/TA-AT-test.xlsx\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T03:22:38.466889Z","iopub.execute_input":"2025-01-29T03:22:38.467628Z","iopub.status.idle":"2025-01-29T03:22:38.987617Z","shell.execute_reply.started":"2025-01-29T03:22:38.467594Z","shell.execute_reply":"2025-01-29T03:22:38.986906Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"## Audio file loading","metadata":{}},{"cell_type":"code","source":"ml_audio_dir = \"/kaggle/input/shared-task-hunt/Train set/tamil/audio\"\nml_audio_test_dir = \"/kaggle/input/shared-task-hunt/Test set/tamil/audio\"\nml_audio_files = [os.path.join(ml_audio_dir, file) for file in os.listdir(ml_audio_dir) if file.endswith(\".wav\")]\nml_audio_test_files = [os.path.join(ml_audio_test_dir, file) for file in os.listdir(ml_audio_test_dir) if file.endswith(\".wav\")]\nml_file_name = [os.path.join(\"\", file) for file in os.listdir(ml_audio_dir) if file.endswith(\".wav\")]\n\nall_audio_files = ml_audio_files\nall_file_name = ml_file_name\nall_audio_test_file = ml_audio_test_files","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T03:22:38.989210Z","iopub.execute_input":"2025-01-29T03:22:38.989899Z","iopub.status.idle":"2025-01-29T03:22:39.009991Z","shell.execute_reply.started":"2025-01-29T03:22:38.989872Z","shell.execute_reply":"2025-01-29T03:22:39.009264Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## Checking the dataset","metadata":{}},{"cell_type":"code","source":"df_train = df_malayalam.copy()\ndf_test = df_test_malayalam.copy()\nprint(f\" The size of the Total dataset {df_train.shape}\")\nprint(f\" The size of the Test dataset {df_test.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T03:22:39.011288Z","iopub.execute_input":"2025-01-29T03:22:39.011542Z","iopub.status.idle":"2025-01-29T03:22:39.016743Z","shell.execute_reply.started":"2025-01-29T03:22:39.011520Z","shell.execute_reply":"2025-01-29T03:22:39.015806Z"}},"outputs":[{"name":"stdout","text":" The size of the Total dataset (514, 3)\n The size of the Test dataset (50, 2)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"df_train.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T03:22:39.017586Z","iopub.execute_input":"2025-01-29T03:22:39.017836Z","iopub.status.idle":"2025-01-29T03:22:39.045471Z","shell.execute_reply.started":"2025-01-29T03:22:39.017806Z","shell.execute_reply":"2025-01-29T03:22:39.044614Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"  Class Label Short             File Name  \\\n0                 C  H_TA_003_C_M_016_005   \n1                 C  H_TA_003_C_M_016_004   \n2                 C  H_TA_003_C_M_016_003   \n3                 C  H_TA_003_C_M_015_002   \n4                 C  H_TA_003_C_M_015_001   \n\n                                          Transcript  \n0  உருவத்தை வச்ச ஒருத்தன் கிண்டல் பண்றான் பாருங்க...  \n1  காமெடி பண்ண சொன்னா ஒருத்தன உருவ கேலி பண்ணிட்டு...  \n2  இந்த உருவத்தை வைத்து கிண்டல் கேலி பண்ணி சிரிக்...  \n3  புரிஞ்சுக்கணும் மேடையில் ஒரு நாகரிகம்னு ஒன்னு ...  \n4  என்னா மல மல அண்ணாமலை இது உலகத்தோட ஸ்டைலு உட்கா...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Class Label Short</th>\n      <th>File Name</th>\n      <th>Transcript</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>C</td>\n      <td>H_TA_003_C_M_016_005</td>\n      <td>உருவத்தை வச்ச ஒருத்தன் கிண்டல் பண்றான் பாருங்க...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>C</td>\n      <td>H_TA_003_C_M_016_004</td>\n      <td>காமெடி பண்ண சொன்னா ஒருத்தன உருவ கேலி பண்ணிட்டு...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>C</td>\n      <td>H_TA_003_C_M_016_003</td>\n      <td>இந்த உருவத்தை வைத்து கிண்டல் கேலி பண்ணி சிரிக்...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>C</td>\n      <td>H_TA_003_C_M_015_002</td>\n      <td>புரிஞ்சுக்கணும் மேடையில் ஒரு நாகரிகம்னு ஒன்னு ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>C</td>\n      <td>H_TA_003_C_M_015_001</td>\n      <td>என்னா மல மல அண்ணாமலை இது உலகத்தோட ஸ்டைலு உட்கா...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"df_test.sample(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T03:22:39.046496Z","iopub.execute_input":"2025-01-29T03:22:39.046756Z","iopub.status.idle":"2025-01-29T03:22:39.058173Z","shell.execute_reply.started":"2025-01-29T03:22:39.046723Z","shell.execute_reply":"2025-01-29T03:22:39.057433Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"    File Name                                         Transcript\n25  TA_TE_046  அவங்க சந்தோஷம் தான் எனக்கு முக்கியம் அவங்க நல்...\n0   TA_TE_041  நாம் அழகா அசிங்கமா அப்படிங்கறது முக்கியம் கிடை...\n22  TA_TE_014  நம்ம ஜெயக்குமார் புரோக்கர் மாமாக்கெல்லாம் மந்த...\n7   TA_TE_033  கும்பகோணம் நம்ம கிரகிக்க முடியாத அளவுக்கு சாத்...\n35  TA_TE_026   இருக்கைக்கு எதிரானது, இதனால உலகம் அழியப்போகிற...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>File Name</th>\n      <th>Transcript</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>25</th>\n      <td>TA_TE_046</td>\n      <td>அவங்க சந்தோஷம் தான் எனக்கு முக்கியம் அவங்க நல்...</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>TA_TE_041</td>\n      <td>நாம் அழகா அசிங்கமா அப்படிங்கறது முக்கியம் கிடை...</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>TA_TE_014</td>\n      <td>நம்ம ஜெயக்குமார் புரோக்கர் மாமாக்கெல்லாம் மந்த...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>TA_TE_033</td>\n      <td>கும்பகோணம் நம்ம கிரகிக்க முடியாத அளவுக்கு சாத்...</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>TA_TE_026</td>\n      <td>இருக்கைக்கு எதிரானது, இதனால உலகம் அழியப்போகிற...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"print(all_audio_files[0])\nprint(all_file_name[0])\nprint(all_audio_test_file[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T03:22:39.059110Z","iopub.execute_input":"2025-01-29T03:22:39.059471Z","iopub.status.idle":"2025-01-29T03:22:39.072987Z","shell.execute_reply.started":"2025-01-29T03:22:39.059439Z","shell.execute_reply":"2025-01-29T03:22:39.072265Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/shared-task-hunt/Train set/tamil/audio/H_TA_003_R_M_003_008.wav\nH_TA_003_R_M_003_008.wav\n/kaggle/input/shared-task-hunt/Test set/tamil/audio/TA_TE_004.wav\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"text_file_name = list(df_train['File Name'])\nonly_file_name = [file.split('/')[7][:-4] for file in all_audio_files]\n\nprint(text_file_name[0])\nprint(only_file_name[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T03:22:39.075323Z","iopub.execute_input":"2025-01-29T03:22:39.075587Z","iopub.status.idle":"2025-01-29T03:22:39.091144Z","shell.execute_reply.started":"2025-01-29T03:22:39.075554Z","shell.execute_reply":"2025-01-29T03:22:39.090380Z"}},"outputs":[{"name":"stdout","text":"H_TA_003_C_M_016_005\nH_TA_003_R_M_003_008\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"## Checking for missing files in audio data","metadata":{}},{"cell_type":"code","source":"for text, audio in zip(text_file_name, only_file_name):\n    if(text not in only_file_name):\n        print(text)\n\nfor text, audio in zip(text_file_name, only_file_name):\n    if(audio not in text_file_name):\n        print(audio)\n\nfor text in text_file_name:\n    if(text not in only_file_name):\n        df_train.drop(df_train[df_train['File Name'] == text].index, inplace = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T03:22:39.092867Z","iopub.execute_input":"2025-01-29T03:22:39.093091Z","iopub.status.idle":"2025-01-29T03:22:39.120211Z","shell.execute_reply.started":"2025-01-29T03:22:39.093072Z","shell.execute_reply":"2025-01-29T03:22:39.119451Z"}},"outputs":[{"name":"stdout","text":"H_TA_002_G_M_037_001\nH_TA_002_G_M_037_002\nH_TA_002_G_M_038_001\nH_TA_002_G_M_038_002\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"## Using back translation for data augmentation","metadata":{}},{"cell_type":"code","source":"translator = Translator()\ndef back_translate(text, src_lang, intermediate_lang=\"en\"):\n    try:\n        translated = translator.translate(text, src=src_lang, dest=intermediate_lang).text\n        back_translated = translator.translate(translated, src=intermediate_lang, dest=src_lang).text\n        return back_translated\n    except Exception as e:\n        print(f\"Error in back translation: {e}\")\n        return text\n\nclasslist = ['C', 'G', 'R', 'P']\n\ntarget = df_train['Class Label Short'].value_counts()['N']\naugmented_texts = []\n\nfor x in classlist:\n    count = df_train['Class Label Short'].value_counts()[x]\n    tmp = target - count\n    start = 0\n    if(x == 'N'):\n        continue\n    while(count != target):\n        for index, row in df_train[df_train['Class Label Short'] == x].iterrows():\n            original_text = row[\"Transcript\"]\n            language = row['File Name'].split('_')[1].lower()\n            \n            back_translated = back_translate(original_text, src_lang=language)\n\n            augmented_texts.append({\"Class Label Short\": row['Class Label Short'], \n                                    \"File Name\": row['File Name'],\n                                    \"Transcript\": back_translated})\n            count = count + 1\n            start = start + 1\n            print(f\"{round(start/tmp*100, 2)} of {x} {count}\")\n            clear_output(wait=True)\n            if(count == target):\n                break\n\n\naugmented_df = pd.DataFrame(augmented_texts)\ndf_train = pd.concat([df_train, augmented_df], ignore_index = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T03:22:39.121143Z","iopub.execute_input":"2025-01-29T03:22:39.121497Z","iopub.status.idle":"2025-01-29T03:31:29.714233Z","shell.execute_reply.started":"2025-01-29T03:22:39.121466Z","shell.execute_reply":"2025-01-29T03:31:29.713646Z"}},"outputs":[{"name":"stdout","text":"100.0 of P 287\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"print(f\"The new shape of train data after augmentation: {df_train.shape}\")\nprint(\"Balanaced classes!\")\nprint(df_train['Class Label Short'].value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T03:31:29.715056Z","iopub.execute_input":"2025-01-29T03:31:29.715321Z","iopub.status.idle":"2025-01-29T03:31:29.721036Z","shell.execute_reply.started":"2025-01-29T03:31:29.715295Z","shell.execute_reply":"2025-01-29T03:31:29.720234Z"}},"outputs":[{"name":"stdout","text":"The new shape of train data after augmentation: (1435, 3)\nBalanaced classes!\nClass Label Short\nC    287\nN    287\nR    287\nP    287\nG    287\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"all_audio_files = []\n\nfor row in df_train.iterrows():\n    \n    lang = 'tamil'\n    file_name = row[1][1]\n    \n    directory = '/kaggle/input/shared-task-hunt/Train set/' + lang + '/audio/' + file_name + '.wav'\n    all_audio_files.append(directory)\n\nprint(\"Getting the audio file names after augmentation...\")\nprint(f\"Number of audios after augmentation: {len(all_audio_files)}\")\nprint(f\"Sample of audio file names: {all_audio_files[0]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T03:31:29.721912Z","iopub.execute_input":"2025-01-29T03:31:29.722174Z","iopub.status.idle":"2025-01-29T03:31:29.818633Z","shell.execute_reply.started":"2025-01-29T03:31:29.722144Z","shell.execute_reply":"2025-01-29T03:31:29.817884Z"}},"outputs":[{"name":"stdout","text":"Getting the audio file names after augmentation...\nNumber of audios after augmentation: 1435\nSample of audio file names: /kaggle/input/shared-task-hunt/Train set/tamil/audio/H_TA_003_C_M_016_005.wav\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"## Label Encoding","metadata":{}},{"cell_type":"code","source":"le = LabelEncoder()\ndf_train['Class Label Short'] = le.fit_transform(df_train['Class Label Short'])\nlabel = list(df_train['Class Label Short'])\n\nencoding_dict = {class_label: index for index, class_label in enumerate(le.classes_)}\nprint(encoding_dict)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T03:31:29.819409Z","iopub.execute_input":"2025-01-29T03:31:29.819650Z","iopub.status.idle":"2025-01-29T03:31:29.825768Z","shell.execute_reply.started":"2025-01-29T03:31:29.819629Z","shell.execute_reply":"2025-01-29T03:31:29.825027Z"}},"outputs":[{"name":"stdout","text":"{'C': 0, 'G': 1, 'N': 2, 'P': 3, 'R': 4}\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"df_train.drop(columns = ['File Name'], inplace = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T03:31:29.826677Z","iopub.execute_input":"2025-01-29T03:31:29.826918Z","iopub.status.idle":"2025-01-29T03:31:29.845394Z","shell.execute_reply.started":"2025-01-29T03:31:29.826898Z","shell.execute_reply":"2025-01-29T03:31:29.844679Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"## Loading audio files","metadata":{}},{"cell_type":"code","source":"data_dict = {\"audio\": all_audio_files, \"label\": label}\ndataset = hgdataset.from_dict(data_dict)\ndataset\n\n\ndata_dict = {\"audio\": all_audio_test_file}\ntest_dataset_audio = hgdataset.from_dict(data_dict)\ntest_dataset_audio\n\naudio_sample = dataset[1]\nprint(f\"audio sample details: \\n \\n {audio_sample}\")\n\ndataset = dataset.cast_column(\"audio\",\n     Audio(sampling_rate=16_000))\nprint(f\"Training set sample {dataset[3]}\\n \\n\")\n\ntest_dataset_audio = test_dataset_audio.cast_column(\"audio\",\n     Audio(sampling_rate=16_000))\nprint(f\"Test set sample {test_dataset_audio[3]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T03:31:29.846180Z","iopub.execute_input":"2025-01-29T03:31:29.846388Z","iopub.status.idle":"2025-01-29T03:31:42.794306Z","shell.execute_reply.started":"2025-01-29T03:31:29.846370Z","shell.execute_reply":"2025-01-29T03:31:42.793462Z"}},"outputs":[{"name":"stdout","text":"audio sample details: \n \n {'audio': '/kaggle/input/shared-task-hunt/Train set/tamil/audio/H_TA_003_C_M_016_004.wav', 'label': 0}\nTraining set sample {'audio': {'path': '/kaggle/input/shared-task-hunt/Train set/tamil/audio/H_TA_003_C_M_015_002.wav', 'array': array([0.0100036 , 0.01816487, 0.01840702, ..., 0.0028185 , 0.00272367,\n       0.0028947 ]), 'sampling_rate': 16000}, 'label': 0}\n \n\nTest set sample {'audio': {'path': '/kaggle/input/shared-task-hunt/Test set/tamil/audio/TA_TE_045.wav', 'array': array([-4.18968815e-09,  2.83883743e-08, -1.88666700e-08, ...,\n       -8.85165669e-03, -1.16978940e-02, -6.09159889e-03]), 'sampling_rate': 16000}}\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"## Audio Augmentation","metadata":{}},{"cell_type":"code","source":"def custom_time_stretch(audio, rate=1.2):\n    \"\"\"Stretch the audio in time.\"\"\"\n    return librosa.effects.time_stretch(y = audio, rate = rate)\n\ndef custom_pitch_shift(audio, sampling_rate, n_steps=2):\n    \"\"\"Shift the pitch of the audio.\"\"\"\n    return librosa.effects.pitch_shift(y=audio, sr=sampling_rate, n_steps=n_steps)\n\ndef add_white_noise(audio, noise_factor=0.005):\n    \"\"\"Add random white noise to the audio.\"\"\"\n    noise = np.random.randn(len(audio))\n    return audio + noise_factor * noise\n\ndef change_volume(audio, factor=1.2):\n    \"\"\"Change the volume of the audio.\"\"\"\n    return audio * factor\n    \ndef augment_audio(audio, sampling_rate):\n    \"\"\"Apply a series of augmentations to the audio.\"\"\"\n    # Randomly choose augmentations\n    aug_choice = random.choice([custom_time_stretch, custom_pitch_shift, add_white_noise, change_volume])\n    \n    if aug_choice == custom_time_stretch:\n        return custom_time_stretch(audio)\n    elif aug_choice == custom_pitch_shift:\n        return custom_pitch_shift(audio, sampling_rate)\n    elif aug_choice == add_white_noise:\n        return add_white_noise(audio)\n    elif aug_choice == change_volume:\n        return change_volume(audio)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T03:31:42.795128Z","iopub.execute_input":"2025-01-29T03:31:42.795716Z","iopub.status.idle":"2025-01-29T03:31:42.802078Z","shell.execute_reply.started":"2025-01-29T03:31:42.795693Z","shell.execute_reply":"2025-01-29T03:31:42.801212Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"wav2vec2_processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T03:31:42.802831Z","iopub.execute_input":"2025-01-29T03:31:42.803079Z","iopub.status.idle":"2025-01-29T03:31:44.905325Z","shell.execute_reply.started":"2025-01-29T03:31:42.803058Z","shell.execute_reply":"2025-01-29T03:31:44.904642Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/159 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c10cbc65461c4714adfc4f05bc2755c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/163 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b9f4d1ac6264122bedfeeb7ea2db955"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.60k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d2211067feb46368f56fcfdb38affc3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/291 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64624ff3a7724ba283ab934b1ea80553"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4cf52d7987f1460884af0c624fde8d73"}},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"MAX_LENGTH = 16000\n\ndef extract_mfcc(audio, sampling_rate=16000, n_mfcc=13, hop_length=512):\n    mfcc = librosa.feature.mfcc(y=audio, sr=sampling_rate, n_mfcc=n_mfcc, hop_length=hop_length)\n    return mfcc.T\n\ndef preprocess_function_wav2vec2(batch, train = True):\n    audio = batch[\"audio\"][\"array\"]\n    sampling_rate = batch[\"audio\"][\"sampling_rate\"]\n\n    if(train):\n        label = batch[\"label\"]\n\n    if(train):\n        augmented_audio = augment_audio(audio, sampling_rate)\n    \n    mfcc_features = extract_mfcc(audio, sampling_rate=sampling_rate)\n\n    max_frames = MAX_LENGTH // 512\n    if mfcc_features.shape[0] > max_frames:\n        mfcc_features = mfcc_features[:max_frames, :]\n    else:\n        mfcc_features = np.pad(mfcc_features, ((0, max_frames - mfcc_features.shape[0]), (0, 0)), \"constant\")\n\n\n    if len(audio) > MAX_LENGTH:\n        audio = audio[:MAX_LENGTH]\n    else:\n        audio = np.pad(audio, (0, MAX_LENGTH - len(audio)), \"constant\")\n\n    inputs = wav2vec2_processor(audio, sampling_rate=sampling_rate, return_tensors=\"pt\", padding=True)\n\n    batch[\"input_values\"] = inputs[\"input_values\"][0].numpy()\n    batch[\"mfcc_features\"] = mfcc_features\n    if(train):\n        batch[\"labels\"] = label\n\n    return batch\n\ntry:\n    processed_dataset_wav2vec2 = dataset.map(preprocess_function_wav2vec2)\nexcept:\n    print(\"file not found\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T03:31:44.906174Z","iopub.execute_input":"2025-01-29T03:31:44.906508Z","iopub.status.idle":"2025-01-29T03:39:19.722992Z","shell.execute_reply.started":"2025-01-29T03:31:44.906476Z","shell.execute_reply":"2025-01-29T03:39:19.721951Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1435 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e0aa66ecef84d198cc42174797449e2"}},"metadata":{}}],"execution_count":18},{"cell_type":"markdown","source":"## Splitting the dataset","metadata":{}},{"cell_type":"code","source":"split_dataset = processed_dataset_wav2vec2.train_test_split(test_size=0.2, seed=42)\nsplit_dataset = DatasetDict({\n    \"train\": split_dataset[\"train\"],\n    \"validation\": split_dataset[\"test\"],\n})\n\nprint(split_dataset)\n\nfrom torch.utils.data import Dataset\n\nclass AudioDataset(Dataset):\n    def __init__(self, dataset):\n        self.dataset = dataset\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        item = self.dataset[idx]\n        input_values = torch.tensor(item[\"input_values\"], dtype=torch.float32)\n        labels = torch.tensor(item[\"labels\"], dtype=torch.long)\n        return {\"input_values\": input_values, \"labels\": labels}\n\ntrain_dataset = AudioDataset(split_dataset[\"train\"])\neval_dataset = AudioDataset(split_dataset[\"validation\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T03:39:19.723817Z","iopub.execute_input":"2025-01-29T03:39:19.724115Z","iopub.status.idle":"2025-01-29T03:39:19.737098Z","shell.execute_reply.started":"2025-01-29T03:39:19.724079Z","shell.execute_reply":"2025-01-29T03:39:19.736188Z"}},"outputs":[{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['audio', 'label', 'input_values', 'mfcc_features', 'labels'],\n        num_rows: 1148\n    })\n    validation: Dataset({\n        features: ['audio', 'label', 'input_values', 'mfcc_features', 'labels'],\n        num_rows: 287\n    })\n})\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"## Getting unimodals for multimodal","metadata":{}},{"cell_type":"code","source":"bert_model = TFBertModel.from_pretrained(\"bert-base-multilingual-cased\")\nwav2vec_model = TFWav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T03:39:19.738024Z","iopub.execute_input":"2025-01-29T03:39:19.738317Z","iopub.status.idle":"2025-01-29T03:39:30.359198Z","shell.execute_reply.started":"2025-01-29T03:39:19.738278Z","shell.execute_reply":"2025-01-29T03:39:30.358422Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dbb745ef91554eb48b54fda1e20997ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0518e15b7834d9c803c26917f16200a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/378M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5382b6f72e30428bb8e2405a2ecf1d6b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ef6d4689b504e7b844e49449f44d4bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"484874d6f1d247cebe7718ef59f7cdd0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1972f02306cc46cc84b493464d2e27ef"}},"metadata":{}}],"execution_count":20},{"cell_type":"markdown","source":"## Callback function for f-1 scores","metadata":{}},{"cell_type":"code","source":"class MacroF1Callback(tf.keras.callbacks.Callback):\n    def __init__(self, train_data, validation_data=None):\n        super(MacroF1Callback, self).__init__()\n        self.train_data = train_data\n        self.validation_data = validation_data\n        self.train_f1_scores = []\n        self.val_f1_scores = []\n        \n    def on_epoch_end(self, epoch, logs={}):\n        # Calculate F1 score for training data\n        train_pred = np.argmax(self.model.predict(self.train_data[0]), axis=1)\n        train_true = self.train_data[1]\n        train_f1 = f1_score(train_true, train_pred, average='macro')\n        self.train_f1_scores.append(train_f1)\n        \n        # Calculate F1 score for validation data if provided\n        if self.validation_data is not None:\n            val_pred = np.argmax(self.model.predict(self.validation_data[0]), axis=1)\n            val_true = self.validation_data[1]\n            val_f1 = f1_score(val_true, val_pred, average='macro')\n            self.val_f1_scores.append(val_f1)\n            print(f'\\nEpoch {epoch+1}:')\n            print(f'Training Macro F1-score: {train_f1:.4f}')\n            print(f'Validation Macro F1-score: {val_f1:.4f}')\n        else:\n            print(f'\\nEpoch {epoch+1}:')\n            print(f'Training Macro F1-score: {train_f1:.4f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T03:39:30.360102Z","iopub.execute_input":"2025-01-29T03:39:30.360441Z","iopub.status.idle":"2025-01-29T03:39:30.367068Z","shell.execute_reply.started":"2025-01-29T03:39:30.360395Z","shell.execute_reply":"2025-01-29T03:39:30.366035Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"## Getting input IDs and mask attention of text","metadata":{}},{"cell_type":"code","source":"def preprocess_text(texts):\n    tokenized = tokenizer(\n        texts,\n        truncation=True,\n        padding='max_length',\n        max_length=512,\n        return_tensors=\"tf\"\n    )\n    return tokenized[\"input_ids\"], tokenized[\"attention_mask\"]\n\ndf_train[\"Transcript\"] = df_train[\"Transcript\"].fillna(\"\").astype(str)\ntext_input_ids, text_attention_mask = preprocess_text(df_train[\"Transcript\"].tolist())\ntext_ids_input = Input(shape=(512,), dtype=tf.int32, name=\"text_ids_input\")\ntext_mask_input = Input(shape=(512,), dtype=tf.int32, name=\"text_mask_attention\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T03:39:30.367997Z","iopub.execute_input":"2025-01-29T03:39:30.368223Z","iopub.status.idle":"2025-01-29T03:39:33.936053Z","shell.execute_reply.started":"2025-01-29T03:39:30.368203Z","shell.execute_reply":"2025-01-29T03:39:33.935396Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"## Getting text features from mBERT","metadata":{}},{"cell_type":"code","source":"def bert_model_wrapper(inputs):\n    input_ids, attention_mask = inputs\n    return bert_model(input_ids=input_ids, attention_mask=attention_mask).pooler_output\ntext_features = Lambda(bert_model_wrapper, name=\"bert_embedding\", output_shape=(768,))([text_ids_input, text_mask_input])\ntext_dense = Dense(256, activation=\"relu\", name=\"text_dense\")(text_features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T03:39:33.938804Z","iopub.execute_input":"2025-01-29T03:39:33.939031Z","iopub.status.idle":"2025-01-29T03:39:34.809897Z","shell.execute_reply.started":"2025-01-29T03:39:33.939012Z","shell.execute_reply":"2025-01-29T03:39:34.809237Z"}},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"## Getting Audio Features","metadata":{}},{"cell_type":"code","source":"audio_input = Input(shape=(16000,), dtype=tf.float32, name=\"audio_input\")\nmfcc_features_input = Input(shape=(31, 13), dtype=tf.float32, name=\"mfcc_input\")\n\ndef get_wav2vec_output(x):\n    return wav2vec_model(x).last_hidden_state\n\nwav2vec_output = Lambda(\n    get_wav2vec_output,\n    output_shape=(None, 768),\n    name=\"wav2vec\"\n)(audio_input)\n\nwav2vec_flattened = Lambda(lambda x: x[:, 0, :], name=\"wav2vec_flattened\")(wav2vec_output)\nmfcc_flattened = Flatten(name=\"mfcc_flattened\")(mfcc_features_input)\ncombined_audio_features = Concatenate(name=\"concat_audio_features\")([wav2vec_flattened, mfcc_flattened])\naudio_dense = Dense(256, activation=\"relu\", name=\"audio_dense\")(combined_audio_features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T03:39:34.810836Z","iopub.execute_input":"2025-01-29T03:39:34.811028Z","iopub.status.idle":"2025-01-29T03:39:34.834112Z","shell.execute_reply.started":"2025-01-29T03:39:34.811010Z","shell.execute_reply":"2025-01-29T03:39:34.833321Z"}},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":"## Late Fusion","metadata":{}},{"cell_type":"code","source":"combined_features = Concatenate(name=\"concat_features\")([audio_dense, text_dense])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T03:39:34.834969Z","iopub.execute_input":"2025-01-29T03:39:34.835262Z","iopub.status.idle":"2025-01-29T03:39:34.840239Z","shell.execute_reply.started":"2025-01-29T03:39:34.835231Z","shell.execute_reply":"2025-01-29T03:39:34.839576Z"}},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":"## Classification Head","metadata":{}},{"cell_type":"code","source":"x = Dense(128, activation=\"relu\", name=\"fc1\")(combined_features)\nx = Dropout(0.5, name=\"dropout\")(x)\noutput = Dense(5, activation=\"softmax\", name=\"output\")(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T03:39:34.840943Z","iopub.execute_input":"2025-01-29T03:39:34.841223Z","iopub.status.idle":"2025-01-29T03:39:34.871799Z","shell.execute_reply.started":"2025-01-29T03:39:34.841191Z","shell.execute_reply":"2025-01-29T03:39:34.871045Z"}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"## Multimodal","metadata":{}},{"cell_type":"code","source":"multimodal_model = Model(\n    inputs=[audio_input, mfcc_features_input, text_ids_input, text_mask_input], \n    outputs=output\n)\n\nmultimodal_model.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n    loss=\"sparse_categorical_crossentropy\",\n    metrics=[\"accuracy\"],\n)\nmultimodal_model.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T03:39:34.872539Z","iopub.execute_input":"2025-01-29T03:39:34.872806Z","iopub.status.idle":"2025-01-29T03:39:34.905954Z","shell.execute_reply.started":"2025-01-29T03:39:34.872785Z","shell.execute_reply":"2025-01-29T03:39:34.905216Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ audio_input (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16000\u001b[0m)          │              \u001b[38;5;34m0\u001b[0m │ -                      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ wav2vec (\u001b[38;5;33mLambda\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ audio_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ mfcc_input (\u001b[38;5;33mInputLayer\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m13\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ -                      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ wav2vec_flattened         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ wav2vec[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n│ (\u001b[38;5;33mLambda\u001b[0m)                  │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ mfcc_flattened (\u001b[38;5;33mFlatten\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m403\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ mfcc_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ text_ids_input            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ -                      │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ text_mask_attention       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ -                      │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ concat_audio_features     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1171\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ wav2vec_flattened[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ mfcc_flattened[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ bert_embedding (\u001b[38;5;33mLambda\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ text_ids_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n│                           │                        │                │ text_mask_attention[\u001b[38;5;34m0\u001b[0m… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ audio_dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m300,032\u001b[0m │ concat_audio_features… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ text_dense (\u001b[38;5;33mDense\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m196,864\u001b[0m │ bert_embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ concat_features           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ audio_dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ text_dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ fc1 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m65,664\u001b[0m │ concat_features[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout (\u001b[38;5;33mDropout\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ fc1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ output (\u001b[38;5;33mDense\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              │            \u001b[38;5;34m645\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ audio_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16000</span>)          │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ wav2vec (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ audio_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ mfcc_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ wav2vec_flattened         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ wav2vec[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)                  │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ mfcc_flattened (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">403</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ mfcc_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ text_ids_input            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ text_mask_attention       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ concat_audio_features     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1171</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ wav2vec_flattened[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ mfcc_flattened[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ bert_embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ text_ids_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n│                           │                        │                │ text_mask_attention[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ audio_dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">300,032</span> │ concat_audio_features… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ text_dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">196,864</span> │ bert_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ concat_features           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ audio_dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ text_dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ fc1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65,664</span> │ concat_features[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ fc1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">645</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m563,205\u001b[0m (2.15 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">563,205</span> (2.15 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m563,205\u001b[0m (2.15 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">563,205</span> (2.15 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}],"execution_count":27},{"cell_type":"markdown","source":"## Getting the train & validation data ready","metadata":{}},{"cell_type":"code","source":"train_inputs = {\n    \"audio_input\": np.stack([x[\"input_values\"] for x in processed_dataset_wav2vec2]),\n    \"mfcc_input\": np.stack([x[\"mfcc_features\"] for x in processed_dataset_wav2vec2]),\n    \"text_ids_input\": text_input_ids,\n    \"text_mask_attention\": text_attention_mask,\n}\n\ntrain_labels = np.array(label)\n\nsplitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n\nfor train_idx, val_idx in splitter.split(train_inputs[\"audio_input\"], train_labels):\n    train_idx = tf.convert_to_tensor(train_idx)\n    val_idx = tf.convert_to_tensor(val_idx)\n    train_inputs_final = {\n        key: tf.gather(value, train_idx) for key, value in train_inputs.items()\n    }\n    val_inputs = {\n        key: tf.gather(value, val_idx) for key, value in train_inputs.items()\n    }\n    train_labels_final = tf.gather(train_labels, train_idx)\n    val_labels = tf.gather(train_labels, val_idx)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T03:39:34.906759Z","iopub.execute_input":"2025-01-29T03:39:34.906960Z","iopub.status.idle":"2025-01-29T03:40:03.633926Z","shell.execute_reply.started":"2025-01-29T03:39:34.906932Z","shell.execute_reply":"2025-01-29T03:40:03.633222Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"print(\"Training set sizes:\")\nfor key, value in train_inputs_final.items():\n    print(f\"{key}: {value.shape}\")\nprint(f\"Training labels: {train_labels_final.shape}\")\n\nprint(\"\\nValidation set sizes:\")\nfor key, value in val_inputs.items():\n    print(f\"{key}: {value.shape}\")\nprint(f\"Validation labels: {val_labels.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T03:40:03.634717Z","iopub.execute_input":"2025-01-29T03:40:03.635018Z","iopub.status.idle":"2025-01-29T03:40:03.641751Z","shell.execute_reply.started":"2025-01-29T03:40:03.634989Z","shell.execute_reply":"2025-01-29T03:40:03.640664Z"}},"outputs":[{"name":"stdout","text":"Training set sizes:\naudio_input: (1148, 16000)\nmfcc_input: (1148, 31, 13)\ntext_ids_input: (1148, 512)\ntext_mask_attention: (1148, 512)\nTraining labels: (1148,)\n\nValidation set sizes:\naudio_input: (287, 16000)\nmfcc_input: (287, 31, 13)\ntext_ids_input: (287, 512)\ntext_mask_attention: (287, 512)\nValidation labels: (287,)\n","output_type":"stream"}],"execution_count":29},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"f1_callback = MacroF1Callback(\n    train_data=(train_inputs_final, train_labels_final),\n    validation_data=(val_inputs, val_labels)\n)\n\nmultimodal_model.fit(\n    train_inputs_final,\n    train_labels_final,\n    batch_size=32,\n    epochs=20,\n    validation_data=(val_inputs, val_labels),\n    callbacks=[f1_callback]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T03:40:03.642566Z","iopub.execute_input":"2025-01-29T03:40:03.642865Z","execution_failed":"2025-01-29T03:40:59.702Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/20\n\u001b[1m26/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 532ms/step - accuracy: 0.2043 - loss: 49.2186","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"## Getting the test data ready","metadata":{}},{"cell_type":"code","source":"test_text_input_ids, test_text_attention_mask = preprocess_text(df_test[\"Transcript\"].tolist())\ntest_processed_dataset_wav2vec2 = test_dataset_audio.map(\n    preprocess_function_wav2vec2,\n    fn_kwargs={'train': False}\n)\n\ntest_inputs = {\n    \"audio_input\": np.stack([x[\"input_values\"] for x in test_processed_dataset_wav2vec2]),\n    \"mfcc_input\": np.stack([x[\"mfcc_features\"] for x in test_processed_dataset_wav2vec2]),\n    \"text_ids_input\": test_text_input_ids,\n    \"text_mask_attention\": test_text_attention_mask,\n}\n\npredictions = multimodal_model.predict(test_inputs)\n\npredicted_classes = np.argmax(predictions, axis=1)\n\nprediction_probabilities = predictions","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-29T03:40:59.703Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class_mapping = {\n    0: 'C',\n    1: 'G',\n    2: 'N',\n    3: 'P',\n    4: 'R'\n}","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-29T03:40:59.703Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Getting the TSV","metadata":{}},{"cell_type":"code","source":"predicted_letters = [class_mapping[pred] for pred in predicted_classes]\n\npredictions_df = pd.DataFrame({\n    'Predicted_Class': predicted_letters\n})\npredictions_df.to_csv('predictions.tsv', sep='\\t', index=False)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-29T03:40:59.703Z"}},"outputs":[],"execution_count":null}]}